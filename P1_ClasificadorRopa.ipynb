{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0UorTVdUhz"
      },
      "source": [
        "### Grupo B04\n",
        "### Miguel Egido Morales, Alfredo Robledano Abasolo, Ana Robledano Abasolo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWbYb_Ejqlms"
      },
      "source": [
        "# P1 AA Configuración y Entrenamiento de una Red de Neuronas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYK5LlXYszma"
      },
      "source": [
        "Utilizaremos la **biblioteca Python Keras** para **clasificar** artículos de ropa.\n",
        "\n",
        "**PROBLEMA:**\n",
        "- Clasificación de imágenes en escala de grises de prendas de ropa (28 x 28 píxeles) en sus 10 categorías (de 0 a 9, guardadas en etiquetas).\n",
        "- Usaremos 60K imágenes de entrenamiento y más de 10K imágenes de prueba\n",
        "- El conjunto de datos MNIST está precargado en Keras en la forma de un conjunto de cuatro matrices Numpy\n",
        "- Algunas muestras\n",
        "- Tenemos las siguientes categorías o **clases** del problema de clasificación de ropa: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot.\n",
        "- Los \"puntos de datos\" son **muestras**\n",
        "- La clase asociado a una muestra específica se llama **etiqueta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt69U75udUh1"
      },
      "source": [
        "Importamos el paquete tensorflow que contiene a la librería keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZRjUdLDqlmt",
        "outputId": "adf21091-7740-45f1-d1ee-b005c4d00f28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__ >= '2.0.0'  # Comprobamos que estamos usando al menos la versión 2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifOEj4QTFDju"
      },
      "source": [
        "El módulo keras.datasets contiene un dataset con imágenes de ropa que usaremos para este proyecto.\\\n",
        "Las imágenes se encuentran convenientemente etiquetadas y en formato mnist.\\\n",
        "A continuación almacenamos en memoria las imágenes de entrenamiento  e imágenes de test (junto con sus etiquetas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA0GDH69qlmu",
        "outputId": "c503ed15-5af3-414b-8d10-7872130361fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> (60000, 28, 28) (60000,)\n",
            "<class 'numpy.ndarray'> (10000, 28, 28) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "print(type(train_images), train_images.shape, train_labels.shape)\n",
        "print(type(test_images), test_images.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7MkuUAZqlmv",
        "outputId": "cbbc5025-b1ef-44ab-c4d0-d51963575558"
      },
      "source": [
        "Observamos que train_images y test_images son numpy.arrays de 3 dimensiones.\\\n",
        "60_000 imágenes de 28x28 pixels para el entrenamiento (60_000 etiquetas).\\\n",
        "10_000 imágenes de 28x28 pixels para test (10_000 etiquetas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lWXf4WHdUh3"
      },
      "source": [
        "Comprobamos que las etiquetas van de 0 a 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLk8C6abdUh3",
        "outputId": "60b91b82-477b-4af8-99bf-1cdaf1e4e16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
          ]
        }
      ],
      "source": [
        "print(set(train_labels))\n",
        "print(set(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp_ygXtzdUh3"
      },
      "source": [
        "Echemos un vistazo a alguna imagen del set de entrenamiento (son numpy arrays 2D)\\\n",
        "Por ejemplo la número 30_000\\\n",
        "Para ello importamos numpy, de forma que podamos cambiar las opciones de impresión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi2DmUJ5qlmv",
        "outputId": "e5ab2119-819f-4536-ef93-81ac51bfbcf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0 118 204 181 175 213 199 168 197 111   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 173 225 185 179 225 158 142 227 173   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 170 229 226 226 233 151 167 234 158   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 194 222 212 226 222 240 218 230 163   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 198 222 210 207 211 207 208 231 147   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 188 220 209 210 211 215 208 230 144   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 183 232 214 220 212 220 213 239 158   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 183 232 217 216 215 219 216 238 160   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 185 231 218 221 215 218 214 238 170   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 188 233 215 220 219 219 216 238 169   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 169 240 219 221 222 228 214 243 114   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 128 255 214 218 206 205 225 233  53   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0 200 229 218 221 219 211 217 228 204   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  34 230 214 217 216 218 218 220 217 231  42   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  94 236 213 219 219 218 215 219 213 234 110   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 167 234 212 218 218 219 212 225 215 231 170   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 200 230 213 216 218 221 211 224 214 228 213   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 192 228 213 216 218 222 213 224 215 221 200   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 202 225 216 214 219 215 219 225 215 219 208   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0 215 224 214 215 221 213 217 220 215 216 216   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  26 232 214 218 213 226 212 216 225 215 211 224  27   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  59 234 214 220 214 225 211 213 227 216 212 228  51   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  67 233 214 221 213 227 209 215 228 216 211 228  56   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  92 234 215 219 215 226 208 214 227 215 211 225  42   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 116 236 215 218 211 230 213 212 229 215 211 228  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 140 231 218 221 221 236 204 219 224 221 219 227 132   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 107 200 193 205 194 217 213 218 248 202 172 227 105   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  21 108 142 146 125 173 141  94 152 154  89   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2, suppress=True, linewidth=145)\n",
        "print(np.matrix(train_images[30_000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_bUss0edUh4"
      },
      "source": [
        "A simple vista ningún problema\\\n",
        "Los valores de la matriz 2D asociada a la imagen están entre 0 y 255.\\\n",
        "Podemos ver su etiqueta asociada, 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a9GvqdTqlmv",
        "outputId": "b1e565ec-9b03-43b3-d9d4-0335b0368154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "print(train_labels[30_000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iUyaT4FdUh4"
      },
      "source": [
        "Viendo la documentación sabemos que se trata de un vestido por ser el valor 3. (3: Dress)\\\n",
        "No obstante podemos usar el paquete matplotlib para ver como es la imagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "vq3OWBIgqlmv",
        "outputId": "29cc78d5-1316-4163-f7ee-d69359ae7b03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ropa = train_images[30000]\n",
        "plt.imshow(ropa, cmap=plt.cm.binary) # el num más bajo se pone en color blanco, y el más alto en negro\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cZYNDGzdUh4"
      },
      "source": [
        "Abrimos también una imagen del set de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ylVHZ8aYqlmw",
        "outputId": "0bed99ec-c4ca-4075-8ec4-14ee2601457c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ropa = test_images[0]\n",
        "plt.imshow(ropa, cmap=plt.cm.binary) # Veamos el elemento 0 del set de tests y pintémoslo con matplotlib\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNu3uJWTdUh4"
      },
      "source": [
        "Un zapato, no cabe duda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmJs2BINqlmw",
        "outputId": "0454a02f-86ca-4124-d5d4-a10e1663b993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ],
      "source": [
        "print(test_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcVJhxOxdUh5"
      },
      "source": [
        "Así lo indica su etiqueta (9: shoe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVDGHuPNvTqL"
      },
      "source": [
        "La función de keras keras.layers.Flatten() nos podría ser útil para aplanar la entrada .... (falta info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzT5-LModUh5"
      },
      "source": [
        "## Cuestiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU9pZNHbXARm"
      },
      "source": [
        "# Normalización de los datos\n",
        "para facilitar que converja el proceso de entrenamiento preparamos los datos de imagen con alguna transformación. Los tensores transformados tienen la misma cantidad de datos total que el tensor inicial.\n",
        "\n",
        "Utilizaremos la función flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMMrQ03KXVpH",
        "outputId": "e40c830f-eb0b-46f4-dfb0-c0e9e51e7c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "[[  0   0   0   0   0   0   0   0   1   1   0   0 120 131  91 147  30   0   0   1   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   1   3   0   0   0   0 251 199 172 195 152   0   0   0   0   3   1   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  43 124 193 166 239 255 216 172 228 126  61   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  96 167 155 159 171 178 211 215 210 196 189 158 164 159 108   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  83 157 131 117 120 148 148 145 178 159 174 160 123 132 142 172  38   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 159 128 118 120 122 112  93 124 161 109 128 128 129 146 138 167 122   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 171 135 120 114 118 119 107 125 123 117 124 124 119 145 147 166 148   0   0   0   0   0]\n",
            " [  0   0   0   0   0   4 171 138 126 120 117 118 102 122 145 111 120 122 120 152 154 155 170   0   0   0   0   0]\n",
            " [  0   0   0   0   0  31 170 129 138 125 113 106 103 118 137 108 135 130 158 182 138 143 186   0   0   0   0   0]\n",
            " [  0   0   0   0   0  58 163 114 137 143 119 103 100 109 118 109 129 134 172 181 131 136 190   0   0   0   0   0]\n",
            " [  0   0   0   0   0  76 160 113 141 148 128 111 101 116 137 111 131 142 167 183 136 120 186  12   0   0   0   0]\n",
            " [  0   0   0   0   0 100 148 108 147 147 134 120 105 116 131 116 136 137 165 192 137 113 187  30   0   0   0   0]\n",
            " [  0   0   0   0   0 114 143 108 158 147 130 125 106 114 122 119 129 134 160 196 136 109 182  51   0   0   0   0]\n",
            " [  0   0   0   0   0 120 140 117 151 148 131 124 109 120 143 120 130 128 159 188 111 108 178  66   0   0   0   0]\n",
            " [  0   0   0   0   0 125 129 128 112 145 140 122 113 118 134 117 132 128 166 157  91 120 170  74   0   0   0   0]\n",
            " [  0   0   0   0   0 126 123 138  74 140 143 124 111 112 126 120 130 129 175 120  88 128 164  91   0   0   0   0]\n",
            " [  0   0   0   0   0 124 123 157  45 145 143 124 113 119 148 122 131 129 183  90  73 137 155  99   0   0   0   0]\n",
            " [  0   0   0   0   0 118 122 167   0 143 149 122 112 118 137 116 132 126 183  73  50 152 147 101   0   0   0   0]\n",
            " [  0   0   0   0   0 111 128 164   0 142 151 122 111 117 132 120 136 125 182  90  18 164 145 107   0   0   0   0]\n",
            " [  0   0   0   0   0 109 132 158   0 146 148 120 108 125 157 120 136 131 176 111   0 164 143 118   0   0   0   0]\n",
            " [  0   0   0   0   0 111 141 140   0 148 149 120 114 123 137 124 137 131 171 135   0 157 147 125   0   0   0   0]\n",
            " [  0   0   0   0   0 111 154 111   0 155 148 118 116 124 143 123 131 129 167 155   0 129 157 129   0   0   0   0]\n",
            " [  0   0   0   0   0 109 155  87   0 157 145 119 117 126 154 126 130 123 161 160   0  97 163 130   0   0   0   0]\n",
            " [  0   0   0   0   0 124 142  54   0 149 141 119 119 124 136 129 126 120 153 175   0  76 145 137   0   0   0   0]\n",
            " [  0   0   0   0   0 136 151  47   0 149 137 119 118 126 143 132 130 123 153 172   0  66 148 154   0   0   0   0]\n",
            " [  0   0   0   0   0 109 174  48   0 154 138 119 117 124 138 130 129 125 159 167   0  58 174 128   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  85 182 147 136 143 158 146 148 153 199  70   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  26  54  72  83  96  85  80  61  14   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "# train_images = train_images.reshape((60000, 28 * 28))  # TODO: eliminar esta linea\n",
        "print(train_images.shape)\n",
        "np.set_printoptions(precision=2, suppress=True, linewidth=145)\n",
        "print(np.matrix(train_images[3000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2THPuJzYi0S",
        "outputId": "7072e8aa-6fb8-4068-9b6f-16a6fc90e81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.47 0.51 0.36 0.58 0.12 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.98 0.78 0.67 0.76 0.6  0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.17 0.49 0.76 0.65 0.94 1.   0.85 0.67 0.89 0.49 0.24 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.38 0.65 0.61 0.62 0.67 0.7  0.83 0.84 0.82 0.77 0.74 0.62 0.64 0.62 0.42 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.33 0.62 0.51 0.46 0.47 0.58 0.58 0.57 0.7  0.62 0.68 0.63 0.48 0.52 0.56 0.67 0.15 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.62 0.5  0.46 0.47 0.48 0.44 0.36 0.49 0.63 0.43 0.5  0.5  0.51 0.57 0.54 0.65 0.48 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.67 0.53 0.47 0.45 0.46 0.47 0.42 0.49 0.48 0.46 0.49 0.49 0.47 0.57 0.58 0.65 0.58 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.02 0.67 0.54 0.49 0.47 0.46 0.46 0.4  0.48 0.57 0.44 0.47 0.48 0.47 0.6  0.6  0.61 0.67 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.12 0.67 0.51 0.54 0.49 0.44 0.42 0.4  0.46 0.54 0.42 0.53 0.51 0.62 0.71 0.54 0.56 0.73 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.23 0.64 0.45 0.54 0.56 0.47 0.4  0.39 0.43 0.46 0.43 0.51 0.53 0.67 0.71 0.51 0.53 0.75 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.3  0.63 0.44 0.55 0.58 0.5  0.44 0.4  0.45 0.54 0.44 0.51 0.56 0.65 0.72 0.53 0.47 0.73 0.05 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.39 0.58 0.42 0.58 0.58 0.53 0.47 0.41 0.45 0.51 0.45 0.53 0.54 0.65 0.75 0.54 0.44 0.73 0.12 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.45 0.56 0.42 0.62 0.58 0.51 0.49 0.42 0.45 0.48 0.47 0.51 0.53 0.63 0.77 0.53 0.43 0.71 0.2  0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.47 0.55 0.46 0.59 0.58 0.51 0.49 0.43 0.47 0.56 0.47 0.51 0.5  0.62 0.74 0.44 0.42 0.7  0.26 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.49 0.51 0.5  0.44 0.57 0.55 0.48 0.44 0.46 0.53 0.46 0.52 0.5  0.65 0.62 0.36 0.47 0.67 0.29 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.49 0.48 0.54 0.29 0.55 0.56 0.49 0.44 0.44 0.49 0.47 0.51 0.51 0.69 0.47 0.35 0.5  0.64 0.36 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.49 0.48 0.62 0.18 0.57 0.56 0.49 0.44 0.47 0.58 0.48 0.51 0.51 0.72 0.35 0.29 0.54 0.61 0.39 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.46 0.48 0.65 0.   0.56 0.58 0.48 0.44 0.46 0.54 0.45 0.52 0.49 0.72 0.29 0.2  0.6  0.58 0.4  0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.44 0.5  0.64 0.   0.56 0.59 0.48 0.44 0.46 0.52 0.47 0.53 0.49 0.71 0.35 0.07 0.64 0.57 0.42 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.43 0.52 0.62 0.   0.57 0.58 0.47 0.42 0.49 0.62 0.47 0.53 0.51 0.69 0.44 0.   0.64 0.56 0.46 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.44 0.55 0.55 0.   0.58 0.58 0.47 0.45 0.48 0.54 0.49 0.54 0.51 0.67 0.53 0.   0.62 0.58 0.49 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.44 0.6  0.44 0.   0.61 0.58 0.46 0.45 0.49 0.56 0.48 0.51 0.51 0.65 0.61 0.   0.51 0.62 0.51 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.43 0.61 0.34 0.   0.62 0.57 0.47 0.46 0.49 0.6  0.49 0.51 0.48 0.63 0.63 0.   0.38 0.64 0.51 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.49 0.56 0.21 0.   0.58 0.55 0.47 0.47 0.49 0.53 0.51 0.49 0.47 0.6  0.69 0.   0.3  0.57 0.54 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.53 0.59 0.18 0.   0.58 0.54 0.47 0.46 0.49 0.56 0.52 0.51 0.48 0.6  0.67 0.   0.26 0.58 0.6  0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.43 0.68 0.19 0.   0.6  0.54 0.47 0.46 0.49 0.54 0.51 0.51 0.49 0.62 0.65 0.   0.23 0.68 0.5  0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.33 0.71 0.58 0.53 0.56 0.62 0.57 0.58 0.6  0.78 0.27 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.1  0.21 0.28 0.33 0.38 0.33 0.31 0.24 0.05 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "train_images = train_images.astype('float32') / 255\n",
        "print(np.matrix(train_images[3000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "alIAz6yMYjcU"
      },
      "outputs": [],
      "source": [
        "test_images = test_images.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tTSAqxNYhY3"
      },
      "source": [
        "Codificaremos categoricamente las etiquetas en one-hot encoding, transformando las etiquetas en un vector de tantos ceros como el número de etiquetas distinta, y que contiene el valor de 1 en el índice que le corresponde al valor de la etiqueta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGqy5GUBYw9r",
        "outputId": "eebddd2c-b7a1-4151-cf20-5e9be65573a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "# Preparación de las etiquetas\n",
        "import numpy as np\n",
        "from keras import utils\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "train_labels[30000] # Posición 0 a 9 donde solo la 3 tiene probabilidad 1.\n",
        "# El número 30000 de entrenamiento es un 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmYxDHVEZEUI"
      },
      "source": [
        "En el array de la etiqueta 30000, el 1 está en la posición 3 ya que se trata del índice 3 (Dress)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7SvF5s6dUh5"
      },
      "source": [
        "# 1. Configurar y entrenar los siguientes modelos de red de neuronas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBIhnqmcdUh5"
      },
      "source": [
        "A continuación establecemos los parámetros de configuración solicitados\\\n",
        "Utilizaremos namedtuples como contenedores de las configuraciones\\\n",
        "Nota: Las configuraciones de este tipo recomendaríamos tenerlas en un script aparte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGe7bDl5dUh5",
        "outputId": "f392882d-0e08-441a-c808-c93917ead28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param_RN(neuronas_capa1=10, funcion_activacion='relu', optimizador='sgd')\n"
          ]
        }
      ],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "# Guardamos las configuraciones en tuplas\n",
        "neuronas_capa1 = (10, 10, 10, 10, 512, 512, 512, 512)\n",
        "funcion_activacion = (\"relu\", \"relu\", \"sigmoid\", \"sigmoid\", \"relu\", \"relu\", \"sigmoid\", \"sigmoid\")\n",
        "optimizador = (\"sgd\", \"rmsprop\", \"sgd\", \"rmsprop\", \"sgd\", \"rmsprop\", \"sgd\", \"rmsprop\")\n",
        "\n",
        "# Usaremos namedtuples como contenedores de las configuraciones anteriores\n",
        "param_rn = namedtuple(\"Param_RN\",\n",
        "                      [\"neuronas_capa1\",\"funcion_activacion\",\"optimizador\"])\n",
        "\n",
        "# Guardamos en una lista las namedtuples\n",
        "rn_configs = [param_rn(*params) for params in zip(neuronas_capa1,\n",
        "                                                  funcion_activacion,\n",
        "                                                  optimizador)]\n",
        "print(rn_configs[0])  # Vemos un ejemplo de que el formato es correcto"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como queremos obtener los mismos resultados para cualquier usuario que ejecute el programa, usamos una seed para inicializar los pesos posteriormente."
      ],
      "metadata": {
        "id": "8XEkTpV-ZPzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer la semilla global\n",
        "seed_value = 42\n",
        "tf.random.set_seed(seed_value)"
      ],
      "metadata": {
        "id": "ZT_tQ9VkZWXk"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEc03ZbRdUh5"
      },
      "source": [
        "Adicionalmente se utilizará:\n",
        "* función de perdida 'categorical_crossentropy'\n",
        "* métrica de precisión\n",
        "* 5 épocas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkAsMhuXdUh5"
      },
      "source": [
        "**CONSTRUIMOS LA RNA**\n",
        "- **Capa** (**layers**) es el **componente básico de las redes neuronales**. => Es un **filtro** de datos (módulo de procesamiento de datos).Entran datos y salen con una forma más útil para el objetivo del problema a resolver. => **Destilación de datos**\n",
        "\n",
        "El tipo de modelo para nuestras redes será secuencial y utilizaremos 2 capas densas (cada neurona está conectada a todas las demás de esa capa).\n",
        "Las redes constarán de dos capas densas:\n",
        "* **Capa 0** es una capa que convierte el input que son matrices 28x28 a un vector (aplana la matriz a un vector).\n",
        "* **Capa 1** contiene el número de neuronas y su función de activación (ambos especificados en el enunciado)\n",
        "* **Capa 2** contiene 10 neuronas (una para cada tipo de ropa) y función de activación softmax.\n",
        "Esta última capa nos servirá para saber como de bien lo ha hecho la red, al devolvernos una matriz de 10 puntuaciones de probabilidad (sumando 1)\n",
        "\n",
        "La puntuación será la probabilidad de que la imagen pertenezca a una de nuestras clases de 10 tipos de prendas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzV0Wi66AGug",
        "outputId": "7b32e432-e8eb-4192-8b6b-631bb71da651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_97\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Capa_0 (Flatten)            (None, 784)               0         \n",
            "                                                                 \n",
            " Capa_1 (Dense)              (None, 10)                7850      \n",
            "                                                                 \n",
            " Capa_2 (Dense)              (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 3.- CONSTRUIMOS LA ARQUITECTURA DE LA RED\n",
        "from keras import models  # importamos de keras las librerías de modelos y de capas (layers)\n",
        "from keras import layers\n",
        "\n",
        "img_shape = train_images.shape[1:]  # Dimensiones entradas (28, 28)\n",
        "networks = []\n",
        "weights_initializer = tf.keras.initializers.GlorotUniform(seed=seed_value) # inicialización de pesos usando GlorotUniform\n",
        "for config in rn_configs:\n",
        "  network = models.Sequential()\n",
        "  # Capa 0 (Convertir el input en 1D)\n",
        "  network.add(layers.Flatten(input_shape=img_shape,\n",
        "                             name='Capa_0'))\n",
        "\n",
        "  # Capa 1 (configuración enunciado)\n",
        "  network.add(layers.Dense(units=config.neuronas_capa1,\n",
        "                           activation=config.funcion_activacion,\n",
        "                           kernel_initializer=weights_initializer, # inicialización de pesos\n",
        "                           name='Capa_1'))\n",
        "\n",
        "  # Capa 2 (10 neuronas y softmax)\n",
        "  network.add(layers.Dense(units=10,\n",
        "                           activation='softmax',\n",
        "                           kernel_initializer=weights_initializer,\n",
        "                           name='Capa_2'))\n",
        "  networks.append(network)\n",
        "\n",
        "# Por ejemplo, vemos la primera network de nuestra lista de networks\n",
        "networks[0].summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos inicializado los pesos aleatoriamente usando Glorot uniform y la seed que hemos definido anteriormente.\n",
        "Para conseguir una convergencia sustancialmente más rápida y una mayor precisión en los resultados, la mejor forma es inicializar los pesos con un inicializador de pesos GlorotNormal o GlorotUniform."
      ],
      "metadata": {
        "id": "n39nSQlgefeq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGNdAEL_EKvg"
      },
      "source": [
        "Observamos que para la primera red neuronal (networks[0]), tenemos un total de 7960 parámetros que se obtienen como resultado de *nº de neuronas x nº de entradas + sesgos* de cada capa ya que las capas son densas.\n",
        "\n",
        "*   **Capa 1:** 10 x 784 + 10 = 7850\n",
        "*   **Capa 2:** 10 x 10 + 10 = 110\n",
        "*   **TOTAL:** 7850 + 110 = 7960"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5s8MCtJRYCz"
      },
      "source": [
        "Para terminar de preparar la red, debemos elegir:\n",
        "- Una **función de pérdida**: utilizaremos la función de pérdida `categorical_crossentropy` que mide la discrepancia entre las predicciones de un modelo y las respuestas reales en problemas de clasificación con múltiples categorías. Calcula la diferencia entre las distribuciones de probabilidad predichas y las verdaderas, utilizando la entropía cruzada como métrica.\n",
        "\n",
        "- Un **optimizador**: dependiendo del caso usaremos los optimizadores `sgd` o `rmsprop`.\n",
        "  \n",
        "\n",
        "1.   SGD (Descenso de Gradiente Estocástico): Actualiza los pesos en dirección opuesta al gradiente de la función de pérdida. \"Estocástico\" significa que utiliza muestras de datos de manera aleatoria para calcular el gradiente, lo que puede ayudar a evitar mínimos locales.\n",
        "2.   RMSprop (Root Mean Square Propagation): Modifica el SGD para adaptarse a tasas de aprendizaje diferentes para cada parámetro. Almacena una media móvil ponderada de los cuadrados de los gradientes anteriores y utiliza esta información para normalizar la tasa de aprendizaje.\n",
        "\n",
        "\n",
        "- **Métricas** para monitorizar durante el entrenamiento y las pruebas. Solo nos preocuparemos por la **precisión** `accuracy` (la fracción de las imágenes que fueron clasificado)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "FZFKNvdaLKnY"
      },
      "outputs": [],
      "source": [
        "# Cargamos el optimizador, la función de pérdida y las métricas\n",
        "for i, config in enumerate(rn_configs):\n",
        "  networks[i].compile(optimizer=config.optimizador,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy']) # si de cada 10 imágenes acierta 8, tiene un accuracy del 80%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrjL3Y6AVfAx"
      },
      "source": [
        "La función de pérdida crossentropy se utiliza como señal\n",
        "de retroalimentación para aprender los tensores de peso y que la fase de\n",
        "entrenamiento intentará minimizar.\\\n",
        "La reducción de la pérdida se produce mediante el descenso de gradiente\n",
        "estocástico minilote, cuyas reglas exactas están gobernadas por el optimizador\n",
        "'rmsprop'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0viqnuRkZV57"
      },
      "source": [
        "# Entrenamiento de las redes neuronales\n",
        "\n",
        "\n",
        "*   Nº de épocas: Usaremos 5 épocas (la red pasará 5 veces por el conjunto de datos) para\n",
        "separar el entrenamiento en 5 fases, dividir el entrenamiento en épocas es útil para el registro y la evaluación periódica\n",
        "*   Tamaño del lote (batch size): tomamos paquetes de 128 imágenes, para calcular la media de las pérdidas y ajustar los parámetros cada 128 imágenes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evitar largo periodo de compilación, ejecutar esta celda para cargar directamente las redes previamente entrenadas y guardadas en el repositorio:"
      ],
      "metadata": {
        "id": "9ettHc6yoD0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Especifica la ruta al archivo del modelo\n",
        "networks_path = 'trained_networks'\n",
        "\n",
        "# Carga el modelo\n",
        "network_loaded_0 = load_model(\"/\".join([networks_path, \"network_0\"]))\n",
        "print(modelo)\n",
        "\n",
        "target = np.expand_dims(test_images[5], axis=0)  # Añadimos una dimensión al np.array\n",
        "# TODO: prediccion de todas las redes\n",
        "probability = network_loaded_0.predict(target)  # Obtenemos los resultados de la capa de salida (probabilidad en cada componente)\n",
        "prediction = np.argmax(probability)  # Tomamos como salida de la red el índice de la categoría con mayor probabilidad\n",
        "\n",
        "print(probability, prediction)"
      ],
      "metadata": {
        "id": "OMd0ZI5roPJQ",
        "outputId": "444f9a46-60d9-41d3-b284-f63b8207ab82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.src.engine.sequential.Sequential object at 0x7c04c0dc5ea0>\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "[[0.   0.97 0.   0.01 0.01 0.   0.01 0.   0.   0.  ]] 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMFjaqJLCU7u",
        "outputId": "33fc74c2-93d6-47da-a4a4-7c770e26c900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5841 - accuracy: 0.7970\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5620 - accuracy: 0.8061\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5449 - accuracy: 0.8113\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5313 - accuracy: 0.8164\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5204 - accuracy: 0.8192\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4096 - accuracy: 0.8577\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4024 - accuracy: 0.8595\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3945 - accuracy: 0.8623\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3893 - accuracy: 0.8636\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3841 - accuracy: 0.8655\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.2124 - accuracy: 0.6531\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.1288 - accuracy: 0.6637\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.0626 - accuracy: 0.6745\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.0097 - accuracy: 0.6826\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.9667 - accuracy: 0.6893\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4652 - accuracy: 0.8428\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4448 - accuracy: 0.8474\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4297 - accuracy: 0.8515\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4184 - accuracy: 0.8543\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4100 - accuracy: 0.8566\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4892 - accuracy: 0.8367\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4752 - accuracy: 0.8402\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4635 - accuracy: 0.8435\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4537 - accuracy: 0.8455\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.4458 - accuracy: 0.8488\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2790 - accuracy: 0.8967\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2660 - accuracy: 0.9005\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2542 - accuracy: 0.9055\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2425 - accuracy: 0.9095\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.2351 - accuracy: 0.9122\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.7184 - accuracy: 0.7612\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.6875 - accuracy: 0.7671\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6629 - accuracy: 0.7747\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.6429 - accuracy: 0.7797\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.6262 - accuracy: 0.7848\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3499 - accuracy: 0.8723\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3371 - accuracy: 0.8780\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3240 - accuracy: 0.8827\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.3139 - accuracy: 0.8853\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.3054 - accuracy: 0.8877\n"
          ]
        }
      ],
      "source": [
        "for i, network in enumerate(networks):\n",
        "    network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBKPt5CNbWR8"
      },
      "source": [
        "La red empezará a iterar por lo datos de entrenamiento en minilotes de 128 muestras, 5 veces. En cada iteración, la red computará los gradientes de los pesos en relación con la pérdida en el lote y ajustará los pesos en\n",
        "consecuencia. Tras estas 5 repeticiones, la red habrá realizado 2.345 ajustes de gradiente (469 por repetición), la pérdida será lo bastante baja como para que la red sea capaz de clasificar números escritos a mano con gran exactitud.\n",
        "\n",
        "Ya que el tiempo de compilación es alto, hemos creado carpetas con las redes neuronales para no tener que compilarlas cada vez que se abra el documento. En su lugar, las cargaremos con la función load y trabajaremos con los resultados obtenidos."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igRsHUjGjK55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yEdnkcjdy3l"
      },
      "source": [
        "# 2. Explicar la salida de la llamada model.summary() de cada uno de los 8 casos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjFsCJv_Nop3",
        "outputId": "5232f575-2517-424b-c9b3-a0a5f9e4d9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7960 (31.09 KB)\n",
            "Trainable params: 7960 (31.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'''for i, network in enumerate(networks):\n",
        "  print(f\"\\nPARÁMETROS DE LA RED NEURONAL {i}:\")\n",
        "  network.summary()\n",
        "  print(\"\\n\")'''\n",
        "networks[0].summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPwy8WFkKaMw"
      },
      "source": [
        "# 3. Analizar e interpretar los resultados del caso 2 y el 7 frente a sus originales si se multiplica por 5 las épocas de entrenamiento (25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXJwaXDwV1XR"
      },
      "source": [
        "Imprimimos los resultados del caso 2 y 7 (con 5 épocas tal y como están programados originalmente)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8_0o6pTQ8ks",
        "outputId": "75530c6f-f596-40fa-9d45-a54a50f8f93f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.3668 - accuracy: 0.1172\n",
            "test_loss: 2.3667678833007812\n",
            "test_acc: 0.11720000207424164\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.5949 - accuracy: 0.1001\n",
            "test_loss: 2.594871759414673\n",
            "test_acc: 0.10010000318288803\n"
          ]
        }
      ],
      "source": [
        "# 8.- VERIFICAMOS NUESTRO MODELO YA ENTRENADO, CONTRA EL CONJUNTO DE PRUEBAS\n",
        "v = [1, 6]\n",
        "for i in v:\n",
        "  test_loss, test_acc = networks[i].evaluate(test_images, test_labels) # la precisión es menor, pq ha perdido generalidad (sobreentrenamiento)\n",
        "  print('test_loss:', test_loss)\n",
        "  print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qd87L85WBR2"
      },
      "source": [
        "Obtenemos una 'accuracy' del 79.51%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFvUHW0QIOR"
      },
      "source": [
        "Multiplicamos por 5 las épocas de entrenamiento y volvemos a entrenar las redes 2 y 7 con el nuevo número de épocas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxO9sZQpWAcf",
        "outputId": "e34a9a75-d765-496f-b92a-6cd7b6ab8d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.8050 - accuracy: 0.7387\n",
            "Epoch 2/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.5299 - accuracy: 0.8205\n",
            "Epoch 3/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.4818 - accuracy: 0.8336\n",
            "Epoch 4/25\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4562 - accuracy: 0.8431\n",
            "Epoch 5/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4374 - accuracy: 0.8491\n",
            "Epoch 6/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4260 - accuracy: 0.8529\n",
            "Epoch 7/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4159 - accuracy: 0.8562\n",
            "Epoch 8/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4090 - accuracy: 0.8591\n",
            "Epoch 9/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4023 - accuracy: 0.8607\n",
            "Epoch 10/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3963 - accuracy: 0.8616\n",
            "Epoch 11/25\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3923 - accuracy: 0.8637\n",
            "Epoch 12/25\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3896 - accuracy: 0.8652\n",
            "Epoch 13/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3860 - accuracy: 0.8663\n",
            "Epoch 14/25\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3825 - accuracy: 0.8668\n",
            "Epoch 15/25\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3805 - accuracy: 0.8674\n",
            "Epoch 16/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3774 - accuracy: 0.8691\n",
            "Epoch 17/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3750 - accuracy: 0.8694\n",
            "Epoch 18/25\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3722 - accuracy: 0.8706\n",
            "Epoch 19/25\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3714 - accuracy: 0.8706\n",
            "Epoch 20/25\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3685 - accuracy: 0.8718\n",
            "Epoch 21/25\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3672 - accuracy: 0.8734\n",
            "Epoch 22/25\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3662 - accuracy: 0.8729\n",
            "Epoch 23/25\n",
            " 10/469 [..............................] - ETA: 2s - loss: 0.3494 - accuracy: 0.8844"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\P1_ClasificadorRopa.ipynb Cell 51\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alfre/Desktop/Utils/P1_RNClasificadorZalando/P1_ClasificadorRopa.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m v:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alfre/Desktop/Utils/P1_RNClasificadorZalando/P1_ClasificadorRopa.ipynb#Y101sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     networks[i]\u001b[39m.\u001b[39;49mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:1774\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1772\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1773\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1774\u001b[0m     \u001b[39mfor\u001b[39;49;00m step \u001b[39min\u001b[39;49;00m data_handler\u001b[39m.\u001b[39;49msteps():\n\u001b[0;32m   1775\u001b[0m         \u001b[39mwith\u001b[39;49;00m tf\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mTrace(\n\u001b[0;32m   1776\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1777\u001b[0m             epoch_num\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m             _r\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1781\u001b[0m         ):\n\u001b[0;32m   1782\u001b[0m             callbacks\u001b[39m.\u001b[39;49mon_train_batch_begin(step)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1411\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1412\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1413\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1414\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1416\u001b[0m )\n\u001b[0;32m   1418\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:687\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    686\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    688\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    689\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:814\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \n\u001b[0;32m    807\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 814\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    815\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:793\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    791\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    796\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    797\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m   record\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    799\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    800\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    801\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:783\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    782\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 783\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    785\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    786\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:589\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    588\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    590\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    591\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    592\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in v:\n",
        "    networks[i].fit(train_images, train_labels, epochs=25, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVy3FXlKXVko"
      },
      "source": [
        "Imprimimos los nuevos resultados de los casos 2 y 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL-emTN9XfoA",
        "outputId": "4b9dc9c8-51b5-4c36-f298-5736c8229479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4212 - accuracy: 0.8493\n",
            "test_loss: 0.4212188124656677\n",
            "test_acc: 0.8493000268936157\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5342 - accuracy: 0.8109\n",
            "test_loss: 0.5341633558273315\n",
            "test_acc: 0.8108999729156494\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in v:\n",
        "  test_loss, test_acc = networks[i].evaluate(test_images, test_labels) # la precisión es menor, pq ha perdido generalidad (sobreentrenamiento)\n",
        "  print('test_loss:', test_loss)\n",
        "  print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNe3IfLZJjc"
      },
      "source": [
        "Observamos que la precisión ha mejorado, al añadir más épocas, las redes neuronales se han ajustado más al set de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYje0MGTZdZm"
      },
      "source": [
        "# 4. Evaluar cada uno de los 8 modelos comparando el rendimiento del modelo en el conjunto de datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXKlVmprZ0J7",
        "outputId": "83c91c2f-8f69-4cbd-db1e-d7d243af5831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5965 - accuracy: 0.7951\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 0:\n",
            "test_loss: 0.5964755415916443\n",
            "test_acc: 0.7950999736785889\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4212 - accuracy: 0.8493\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 1:\n",
            "test_loss: 0.4212188124656677\n",
            "test_acc: 0.8493000268936157\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.3840 - accuracy: 0.1237\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 2:\n",
            "test_loss: 2.383974313735962\n",
            "test_acc: 0.12370000034570694\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4363 - accuracy: 0.8469\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 3:\n",
            "test_loss: 0.43632417917251587\n",
            "test_acc: 0.8468999862670898\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.5081 - accuracy: 0.0944\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 4:\n",
            "test_loss: 2.5080924034118652\n",
            "test_acc: 0.09440000355243683\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3011 - accuracy: 0.1531\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 5:\n",
            "test_loss: 2.3010852336883545\n",
            "test_acc: 0.15309999883174896\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5342 - accuracy: 0.8109\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 6:\n",
            "test_loss: 0.5341633558273315\n",
            "test_acc: 0.8108999729156494\n",
            "\n",
            "\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.5081 - accuracy: 0.1002\n",
            "\n",
            "PARÁMETROS DE LA RED NEURONAL 7:\n",
            "test_loss: 2.5081284046173096\n",
            "test_acc: 0.10019999742507935\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, network in enumerate(networks):\n",
        "  test_loss, test_acc = network.evaluate(test_images, test_labels) # la precisión es menor, pq ha perdido generalidad (sobreentrenamiento)\n",
        "  print(f\"\\nPARÁMETROS DE LA RED NEURONAL {i}:\")\n",
        "  print('test_loss:', test_loss)\n",
        "  print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3HA9sqGaj3h"
      },
      "source": [
        "# 5. Usar cada uno de los 8 modelos para hacer predicciones sobre la 6ª imagen de test (test_images[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dQ0NtqROkRm"
      },
      "source": [
        "Las redes esperan un set de datos en la forma np.array de tres dimensiones, por tanto no le puedo pasar test_images[5] que es un np.array de (28, 28), es decir, dos dimensiones.\\\n",
        "En palabras simples la red espera un vector con imágenes dentro, en total espera vectores con 3 dimensiones.\\\n",
        "Para pasarle una sola imagen la tenemos que meter en un vector, que equivale a añadir una dimensión en la componente 0 (axis=0).\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8HT1eF77ZqN"
      },
      "outputs": [],
      "source": [
        "target = np.expand_dims(test_images[5], axis=0)  # Añadimos una dimensión al np.array\n",
        "# TODO: prediccion de todas las redes\n",
        "probability = networks[0].predict(target)  # Obtenemos los resultados de la capa de salida (probabilidad en cada componente)\n",
        "prediction = np.argmax(probability)  # Tomamos como salida de la red el índice de la categoría con mayor probabilidad\n",
        "\n",
        "print(probability, prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ-feL8CbK2I"
      },
      "source": [
        "# 7. Mejoras al modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cfuHEA2cAJu"
      },
      "source": [
        "Usando el de la configuración del caso 3, pero cambiando el\n",
        "optimizador por ‘adam’ y la función de pérdida ‘sparse_categorical_crossentropy’. Buscar en internet las bases de dicho optimizador y función de pérdida, explícalos con tus propias palabras y plantea tus reflexiones respecto al resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK2s2NamcU24"
      },
      "source": [
        "Usaremos la red neuronal 2 (la 3 empezando a contar desde 0), pero con cambios en optimizador y la función de pérdida.\n",
        "* `adam`\n",
        "* `sparse_categorical_crossentropy`\n",
        "\n",
        "**Adam** es un optimizador que combina conceptos de RMSprop y Momentum, manteniendo dos momentos para cada parámetro. Estos momentos permiten que Adam ajuste dinámicamente la tasa de aprendizaje para cada parámetro, adaptándose bien a diferentes conjuntos de datos y arquitecturas de red. La capacidad de Adam para adaptarse automáticamente y su eficiencia en la práctica lo hacen ampliamente utilizado en la optimización de redes neuronales.\n",
        "\n",
        "**sparse_categorical_crossentropy** es una función de pérdida utilizada comúnmente en problemas de clasificación donde las etiquetas son enteros en lugar de codificación one-hot. En lugar de requerir que las etiquetas de destino sean un vector categórico, como en categorical_crossentropy, esta función acepta enteros que representan las clases. Es eficiente y evita la necesidad de convertir las etiquetas en codificación one-hot, simplificando así el proceso. La función calcula la pérdida entre las distribuciones de probabilidad predichas y las etiquetas, siendo adecuada para problemas de clasificación con varias clases donde cada instancia pertenece a una única clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYetFV6YdbHA",
        "outputId": "5e8ccc65-a159-4d68-e580-0d2018f2fecb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'networks' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\alfre\\Desktop\\Utils\\P1_RNClasificadorZalando\\P1_ClasificadorRopa.ipynb Cell 63\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alfre/Desktop/Utils/P1_RNClasificadorZalando/P1_ClasificadorRopa.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m networks[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alfre/Desktop/Utils/P1_RNClasificadorZalando/P1_ClasificadorRopa.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                     loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alfre/Desktop/Utils/P1_RNClasificadorZalando/P1_ClasificadorRopa.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m# si de cada 10 imágenes acierta 8, tiene un accuracy del 80%\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'networks' is not defined"
          ]
        }
      ],
      "source": [
        "networks[2].compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy']) # si de cada 10 imágenes acierta 8, tiene un accuracy del 80%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARJIFLX16_4c"
      },
      "source": [
        "Entrenamos la red con los nuevos cambios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPwkGqwg7EQs",
        "outputId": "7e0761bc-32fc-4828-d77d-7f5557fbec53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-b2201f5b7234>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-49-b2201f5b7234>\", line 1, in <cell line: 1>\n      networks[2].fit(train_images, train_labels, epochs=5, batch_size=128)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2354, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5762, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [128,10] and labels shape [1280]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_148994]"
          ]
        }
      ],
      "source": [
        "networks[2].fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}